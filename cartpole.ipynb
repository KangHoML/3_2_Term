{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DQN을 활용한 CartPole**\n",
    "Q_Learning을 딥러닝에 조합한 방법으로 \"상태\"를 입력으로 받아 \"행동\"을 출력하며 이때 출력은 특정한 상태에서 특정한 행동을 선택할 확률을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow를 Pytorch로 변환\n",
    "import gym # 강화학습을 위한 라이브러리 (gym 0.26.2)\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from collections import deque\n",
    "from torch.nn import HuberLoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 부분은 argparser로 수정 가능하도록 코드 수정 필요\n",
    "# Parameter\n",
    "NUM_EPSIODES = 500\n",
    "MAX_STEPS = 200\n",
    "GAMMA = 0.99\n",
    "WARMUP = 10 # 초기화 시 조작하지 않을 스텝 수\n",
    "\n",
    "# Parameter for search\n",
    "E_INITIAL = 1.0\n",
    "E_STOP = 0.01\n",
    "E_DECAY_RATE = 0.001\n",
    "\n",
    "# Parameter for memory\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        model_blk = []\n",
    "        in_channel = state_size\n",
    "        for _ in range(3):\n",
    "            model_blk.append(\n",
    "                nn.Linear(in_channel, 16)\n",
    "            )\n",
    "            model_blk.append(\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            in_channel = 16\n",
    "\n",
    "        model_blk.append(\n",
    "            nn.Linear(in_channel, action_size)\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(*model_blk)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과거의 경험을 저장하며 오래된 순서부터 제거\n",
    "class Memory():\n",
    "    def __init__(self, memory_size):\n",
    "        self.buffer = deque(maxlen=memory_size) # maxlen 이상의 입력이 들어올 경우 맨 처음 element부터 삭제\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False) # 과거의 경험 중 batch_size 만큼의 경험을 랜덤하게 가져오는 함수\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    printr(f'\\nDevice : {device}')\n",
    "\n",
    "    print('\\nSet & Initialize Envieronment')\n",
    "    env = gym.make('CartPole-v0')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    print('\\nSet Model')\n",
    "    main_net = QNetwork(state_size, action_size) # 갱신 대상 네트워크\n",
    "    target_net = QNetwork(state_size, action_size) # 갱신량 계산을 위한 네트워크 (과거의 메인 네트워크로 일정 에피소드마다 main에 덮어씌움)\n",
    "    memory = Memory(MEMORY_SIZE) # 경험을 저장하기 위한 memory\n",
    "\n",
    "    print('\\nTrain the model')\n",
    "    total_step = 0 # 총 스텝 수\n",
    "    success_count = 0 # 성공 수\n",
    "\n",
    "    for episode in range(1, NUM_EPSIODES+1): # 진행할 episode 수\n",
    "        step = 0\n",
    "        target_net.model.set_weights(main_net.model.get_weights()) # 매 episode마다 target_net 갱신\n",
    "\n",
    "        for _ in range(1, MAX_STEPS+1): # 1 episode 당 업데이트하는 step 수\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "\n",
    "            # ε 감소\n",
    "            epsilon = E_STOP + (E_INITIAL - E_STOP) * np.exp(-E_DECAY_RATE*total_step)\n",
    "\n",
    "            # 행동 선택\n",
    "            if epsilon > np.random.rand():\n",
    "                action = env.action_space.sample() # 랜덤하게 선택\n",
    "            else:\n",
    "                action = np.argmax(main_net.model.predict(state)[0]) # 행동 가치에 따른 선택\n",
    "\n",
    "            # state와 action 저장\n",
    "            next_state, _, done, _ = env.step(action) # done : 에피소드가 완료되었는가\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            # 1 episode 완료 시\n",
    "            if done:\n",
    "                if step >= 190:\n",
    "                    success_count += 1\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    success_count = 0\n",
    "                    reward = 0\n",
    "                \n",
    "                next_state = np.zeros(state.shape) # next_state 초기화\n",
    "\n",
    "                if step > WARMUP: # WARMUP에 해당하는 step마다 memory에 상태 저장\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "                if step > WARMUP:\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # main_net 갱신\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
